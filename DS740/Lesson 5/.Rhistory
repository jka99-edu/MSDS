print(paste("The mean BikeTime for repeat athletes is",r_overall_mean),2)
print(paste("The mean BikeTime for repeat athletes is",nr_overall_mean),2)
ggplot(data = IM_21, aes(x = SwimTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Swim Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = BikeTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Bike Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
ggplot(data = IM_21, aes(x = RunTime, y = OverallTime, color = Group))+
geom_point() +
geom_smooth(method = lm, se = F) +
theme(axis.title = element_text(), legend.position = "none") +
labs(x = "2020 Run Time (minutes)", y = "2021 Overall Time (minutes)") +
stat_regline_equation(label.y = 1000, aes(label = ..adj.rr.label..)) +
facet_wrap(~Group)
gghistogram(IM_21, x = "OverallTime",
add = "mean", rug = F,
color = "Group", fill = "Group")
A <- 10
?mhv()
library(MVTests)
?mhz
library(MASS)  #help(qda)
library(pROC)
# for assumption-checking
library(mvnormalTest)  # of multivariate normality; or library(MVN) or (mvnTest)
library(MVTests)  # of constant covariance; or library(biotools) or (biotools) or (heplots)
# for data organization
library(dplyr)
# for visuals
library(ggformula)
?mhz
############# libraries used ############
# for methods
library(glmnet)  #help(glmnet)
# for assumption-checking
# for data organization
library(dplyr)
# for visuals
library(plotmo)
library(ggplot2)
############# libraries used ############
# for methods
library(glmnet)  #help(glmnet)
# for assumption-checking
# for data organization
library(dplyr)
# for visuals
library(plotmo)
library(ggplot2)
############# Review data #############
#make data frame from original data set
data(trees)
#View(trees)
names(trees)
#define a new dataframe with the variables we want, response at the beginning
TreesTransformed = trees[,c(3,1,2)]
names(TreesTransformed)
# add (using mutate) additional variables to reflect physical characteristics
TreesTransformed <- TreesTransformed %>%
mutate(GirthHeight=trees$Girth*trees$Height,
Girth2=trees$Girth^2,
Girth2Height=trees$Girth^2*trees$Height)
# predictor matrix and response
x = model.matrix(Volume~.,data=TreesTransformed)[,-1]
y = TreesTransformed$Volume
n = dim(x)[1]
p = dim(x)[2]
############# Model fitting #############
# fit multiple regression using all predictors
REGfit = lm(y~x)
summary(REGfit)
# fit penalized regression using all predictors for many different lambda values
lambdalist = exp((-100:100)/10)
# fit ridge regression - use alpha = 0
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
plot_glmnet(RRfit)
plot_glmnet(RRfit,xvar = "lambda")  # display the x-scale for lambda in increasing order
RRlambdaused = 1; abline(v=log(RRlambdaused)) # so log(RRlambdaused) = 0
RRcoef = coef(RRfit,s=RRlambdaused); RRcoef
# fit LASSO - use alpha = 1
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
plot_glmnet(LASSOfit,xvar = "lambda")
LASSOlambdaused = 0.2; abline(v=log(LASSOlambdaused))
LASSOcoef = coef(LASSOfit,s=LASSOlambdaused); LASSOcoef
# fit ENET - use alpha between 0 to 1
ENETfit = glmnet(x, y, alpha = 0.75,lambda=lambdalist)
plot_glmnet(ENETfit,xvar = "lambda")
ENETlambdaused = 0.4; abline(v=log(ENETlambdaused))
ENETcoef = coef(ENETfit,s=ENETlambdaused); ENETcoef
# compare true (observed) y values to predicted values
RRyhat = predict(RRfit,newx=x,s=RRlambdaused)
LASSOyhat = predict(LASSOfit,newx=x,s=LASSOlambdaused)
ENETyhat = predict(ENETfit,newx=x,s=ENETlambdaused)
obs.pred = data.frame(yobs = rep(y,3),
yhat = c(RRyhat,LASSOyhat,ENETyhat),
model = c(rep("RR",n),rep("LASSO",n),rep("ENET",n))
)
ggplot(obs.pred, aes(x=yhat, y=yobs, color=model,shape=model,alpha=0.8)) +
geom_point(size=3)
# compare coefficients
allcoef = data.frame(RRcoef=round(RRcoef[,1],6),
LASSOcoef=round(LASSOcoef[,1],6),
ENETcoef=round(ENETcoef[,1],6))
allcoef
library(dplyr)
library(glmnet)
library(MASS)
library(plotmo)
library(ggplot2)
library(boot)
### Problem 4
# Read in and Review data
BostonNew <- Boston %>%
mutate(log.crim = log(crim), log.zn = log(zn + 1))
# remove crim and zn from BostonNew
BostonNew <- BostonNew[, -c(1, 2)]
x = model.matrix(log.crim~.,data=BostonNew)[,-1]
y = BostonNew$log.crim
n = dim(BostonNew)[1]
p = dim(BostonNew)[2]
lambdalist = (1:1000/1000)
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
RRcoef = coef(RRfit,s=0.05); RRcoef
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
LASSOcoef = coef(LASSOfit,s=0.05); LASSOcoef
ENETfit = glmnet(x, y, alpha = 0.5,lambda=lambdalist)
ENETcoef = coef(ENETfit,s=0.05); ENETcoef
# Problem 5 - (rewatch presentation 2 from lesson 2 for bootstrap refresser)
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 1, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
coef(RRfit,s=RRbestlambda)
coef(LASSOfit,s=LASSObestlambda)
coef(RRfit,s=RRbestlambda)
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 1, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
LASSObestlambda = cvRR$lambda.min
LASSObestlambda
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
ENET50bestlambda = cvRR$lambda.min
ENET50bestlambda
coef(LASSOfit,s=LASSObestlambda)
coef(ENET50fit,s=ENET50bestlambda)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
ENET50bestlambda = cvRR$lambda.min
ENET50bestlambda
coef(ENET50fit,s=ENET50bestlambda)
ENET50fit = glmnet(x, y, alpha = 0.5,lambda=lambdalist)
coef(ENET50fit,s=ENET50bestlambda)
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
cvLASSO = cv.glmnet(x, y, lambda=lambdalist, alpha = 1, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
LASSObestlambda = cvRR$lambda.min
LASSObestlambda
cvENET = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
ENET50bestlambda = cvRR$lambda.min
ENET50bestlambda
coef(ENET50fit,s=ENET50bestlambda)
library(dplyr)
library(glmnet)
library(MASS)
library(plotmo)
library(ggplot2)
library(boot)
### Problem 4
# Read in and Review data
BostonNew <- Boston %>%
mutate(log.crim = log(crim), log.zn = log(zn + 1))
# remove crim and zn from BostonNew
BostonNew <- BostonNew[, -c(1, 2)]
x = model.matrix(log.crim~.,data=BostonNew)[,-1]
y = BostonNew$log.crim
n = dim(BostonNew)[1]
p = dim(BostonNew)[2]
lambdalist = (1:1000/1000)
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
RRcoef = coef(RRfit,s=0.05); RRcoef
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
LASSOcoef = coef(LASSOfit,s=0.05); LASSOcoef
ENET50fit = glmnet(x, y, alpha = 0.5,lambda=lambdalist)
ENETcoef = coef(ENETfit,s=0.05); ENETcoef
# Problem 5 - (rewatch presentation 2 from lesson 2 for bootstrap refresser)
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
cvLASSO = cv.glmnet(x, y, lambda=lambdalist, alpha = 1, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
LASSObestlambda = cvRR$lambda.min
LASSObestlambda
cvENET = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
ENET50bestlambda = cvRR$lambda.min
ENET50bestlambda
coef(RRfit,s=RRbestlambda)
coef(LASSOfit,s=LASSObestlambda)
coef(ENET50fit,s=ENET50bestlambda)
library(dplyr)
library(glmnet)
library(MASS)
library(plotmo)
library(ggplot2)
library(boot)
### Problem 4
# Read in and Review data
BostonNew <- Boston %>%
mutate(log.crim = log(crim), log.zn = log(zn + 1))
# remove crim and zn from BostonNew
BostonNew <- BostonNew[, -c(1, 2)]
x = model.matrix(log.crim~.,data=BostonNew)[,-1]
y = BostonNew$log.crim
n = dim(BostonNew)[1]
p = dim(BostonNew)[2]
lambdalist = (1:1000/1000)
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
RRcoef = coef(RRfit,s=0.05); RRcoef
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
LASSOcoef = coef(LASSOfit,s=0.05); LASSOcoef
ENET50fit = glmnet(x, y, alpha = 0.5,lambda=lambdalist)
ENETcoef = coef(ENETfit,s=0.05); ENETcoef
# Problem 5 - (rewatch presentation 2 from lesson 2 for bootstrap refresser)
n=506
ncv = 10
groups=rep(1:10,length=n)
set.seed(5)
cvgroups = sample(groups,n)
cvRR = cv.glmnet(x, y, lambda=lambdalist, alpha = 0, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
RRbestlambda = cvRR$lambda.min
RRbestlambda
cvLASSO = cv.glmnet(x, y, lambda=lambdalist, alpha = 1, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
LASSObestlambda = cvLASSO$lambda.min
LASSObestlambda
cvENET = cv.glmnet(x, y, lambda=lambdalist, alpha = 0.5, nfolds=ncv, foldid=cvgroups)
min(cvRR$cvm)
ENET50bestlambda = cvENET$lambda.min
ENET50bestlambda
coef(RRfit,s=RRbestlambda)
coef(LASSOfit,s=LASSObestlambda)
coef(ENET50fit,s=ENET50bestlambda)
beta.fn.penalized = function(yxdata,index,bestlambda,lambdalist,alpha) {
yboot = yxdata[index,1]
xboot = yxdata[index,-1]
penfitboot = glmnet(xboot, yboot, alpha = alpha,lambda=lambdalist)
return(coef(penfitboot,s=bestlambda)[,1])
}
library(boot)
set.seed(5)
RRbootoutput = boot(cbind(y,x),beta.fn.penalized,R=1000,
bestlambda=0.012, lambdalist=lambdalist, alpha=0)
print(RRbootoutput)
set.seed(5)
RRbootoutput = boot(cbind(y,x),beta.fn.penalized,R=1000,
bestlambda=0.015, lambdalist=lambdalist, alpha=1)
print(RRbootoutput)
set.seed(5)
RRbootoutput = boot(cbind(y,x),beta.fn.penalized,R=1000,
bestlambda=0.028, lambdalist=lambdalist, alpha=0.5)
print(RRbootoutput)
library(ISLR)
?College
?corrplot
library(corrplot)
library(dplyr)
library(glmnet)
library(ISLR)
library(ggplot2)
# Read in and Review data
Trees <- read.csv("TreesTransformed.csv")
getwd()
setwd("C:/Users/jeffe/Documents/MSDS/GitHub/MSDS/DS740/Lesson 5")
library(dplyr)
library(glmnet)
library(ISLR)
library(ggplot2)
# Read in and Review data
Trees <- read.csv("TreesTransformed.csv")
x = model.matrix(Volume~.,data=Trees)[,-1]
y = Trees$Volume
lambdalist = c((1:100)/100)
# question 1
MLfit = lm(y~x)
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
ENETfit = glmnet(x, y, alpha = 0.7,lambda=lambdalist)
# question 2
summary(MLfit)
# question 3
answer3 <- "Of the five predictor variables in the set, three are transformations of the other two. When taken all into account together, they remove their own significance. "
# question 4
# Ridge Regression
# LASSO
# question 5
LASSOcoef = coef(LASSOfit,s=0.1); LASSOcoef
# question 6
# Height
# Girth * Height
# Girth^2 * Height
# question 7
# LASSO
# question 8
0.35
# question 9
head(College)
answer9 <- "The skewness of the variables Enroll, Apps, Accept, F.Undergrad, and P.Undergrad in the College dataset makes sense because colleges in the United States vary greatly in size. There are very small colleges with only a few hundred students, and there are very large colleges with tens of thousands of students.
The variables Enroll, Apps, Accept, F.Undergrad, and P.Undergrad are all measures of the size of a college, and as such, they are expected to have a wide range of values. Since there are relatively few very large colleges in the dataset compared to smaller colleges, the distribution of these variables will be right-skewed. This means that there will be more smaller colleges with lower values of these variables, and relatively fewer larger colleges with higher values."
# question 10
CollegeT <- College %>%
mutate(Private = as.factor(Private),
log.Enroll = log(Enroll),
log.Apps = log(Apps),
log.Accept = log(Accept),
log.F.Undergrad = log(F.Undergrad),
log.P.Undergrad = log(P.Undergrad)) %>%
select(-Enroll, -Apps, -Accept, -F.Undergrad, -P.Undergrad)
library(dplyr)
library(glmnet)
library(ISLR)
library(ggplot2)
# Read in and Review data
Trees <- read.csv("TreesTransformed.csv")
x = model.matrix(Volume~.,data=Trees)[,-1]
y = Trees$Volume
lambdalist = c((1:100)/100)
# question 1
MLfit = lm(y~x)
RRfit = glmnet(x, y, alpha = 0,lambda=lambdalist)
LASSOfit = glmnet(x, y, alpha = 1,lambda=lambdalist)
ENETfit = glmnet(x, y, alpha = 0.7,lambda=lambdalist)
# question 2
summary(MLfit)
# question 3
answer3 <- "Of the five predictor variables in the set, three are transformations of the other two. When taken all into account together, they remove their own significance. "
# question 4
# Ridge Regression
# LASSO
# question 5
LASSOcoef = coef(LASSOfit,s=0.1); LASSOcoef
# question 6
# Height
# Girth * Height
# Girth^2 * Height
# question 7
# LASSO
# question 8
0.35
# question 9
head(College)
answer9 <- "The skewness of the variables Enroll, Apps, Accept, F.Undergrad, and P.Undergrad in the College dataset makes sense because colleges in the United States vary greatly in size. There are very small colleges with only a few hundred students, and there are very large colleges with tens of thousands of students.
The variables Enroll, Apps, Accept, F.Undergrad, and P.Undergrad are all measures of the size of a college, and as such, they are expected to have a wide range of values. Since there are relatively few very large colleges in the dataset compared to smaller colleges, the distribution of these variables will be right-skewed. This means that there will be more smaller colleges with lower values of these variables, and relatively fewer larger colleges with higher values."
# question 10
CollegeT <- College %>%
mutate(Private = as.factor(Private),
log.Enroll = log(Enroll),
log.Apps = log(Apps),
log.Accept = log(Accept),
log.F.Undergrad = log(F.Undergrad),
log.P.Undergrad = log(P.Undergrad)) %>%
dplyr::select(-Enroll, -Apps, -Accept, -F.Undergrad, -P.Undergrad)
# question 11
library(corrplot)
library(RColorBrewer)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
corrplot(Coll.matrix, method = "eclipse", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
corrplot(Coll.matrix, method = "ellipse", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
# question 10
CollegeT <- College %>%
mutate(Private = ifelse(Private == "Yes", 1, 0),
log.Enroll = log(Enroll),
log.Apps = log(Apps),
log.Accept = log(Accept),
log.F.Undergrad = log(F.Undergrad),
log.P.Undergrad = log(P.Undergrad)) %>%
dplyr::select(-Enroll, -Apps, -Accept, -F.Undergrad, -P.Undergrad)
# question 11
library(corrplot)
library(RColorBrewer)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "ellipse", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
Coll.matrix <- cor(CollegeT)
corrplot(Coll.matrix, method = "ellipse", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
library(corrplot)
library(RColorBrewer)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "ellipse", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")))
Coll.matrix <- cor(CollegeT[,-1], addCoef.col = 1, number.cex = 0.5)
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
Coll.matrix <- cor(CollegeT)
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
# question 10
CollegeT <- College %>%
mutate(Private = as.factor(ifelse(Private == "Yes", 1, 0)),
log.Enroll = log(Enroll),
log.Apps = log(Apps),
log.Accept = log(Accept),
log.F.Undergrad = log(F.Undergrad),
log.P.Undergrad = log(P.Undergrad)) %>%
dplyr::select(-Enroll, -Apps, -Accept, -F.Undergrad, -P.Undergrad)
# question 11
library(corrplot)
library(RColorBrewer)
Coll.matrix <- cor(CollegeT)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "number", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
corrplot(Coll.matrix, method = "square", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
corrplot(Coll.matrix, method = "circle", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
# question 10
CollegeT <- College %>%
mutate(Private = ifelse(Private == "Yes", 1, 0),
log.Enroll = log(Enroll),
log.Apps = log(Apps),
log.Accept = log(Accept),
log.F.Undergrad = log(F.Undergrad),
log.P.Undergrad = log(P.Undergrad)) %>%
dplyr::select(-Enroll, -Apps, -Accept, -F.Undergrad, -P.Undergrad)
# question 11
library(corrplot)
library(RColorBrewer)
Coll.matrix <- cor(CollegeT[,-1])
corrplot(Coll.matrix, method = "circle", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
Coll.matrix <- cor(CollegeT)
corrplot(Coll.matrix, method = "circle", type = "upper",
col = rev(brewer.pal(n = 8 , name = "RdYlBu")), addCoef.col = 1, number.cex = 0.5)
?college
?College
summary(College)
